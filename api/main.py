from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import numpy as np
import gdown
import os
import io
import uvicorn
from typing import Dict, List
import logging
import threading
import time
import hashlib
import gc
import psutil

# Configurar TensorFlow antes de importar - FOR√áAR CPU APENAS
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suprimir todos os logs do TensorFlow
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Desabilitar completamente CUDA/GPU
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false'  # Desabilitar GPU
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Desabilitar otimiza√ß√µes OneDNN
os.environ['TF_DISABLE_MKL'] = '1'  # Desabilitar MKL que pode causar problemas
os.environ['TF_NUM_INTEROP_THREADS'] = '1'  # Limitar threads
os.environ['TF_NUM_INTRAOP_THREADS'] = '1'  # Limitar threads
os.environ['OMP_NUM_THREADS'] = '1'  # Limitar OpenMP threads

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Importar m√≥dulo de retreinamento (ap√≥s configurar TensorFlow)
try:
    from retrain_endpoint import add_retrain_endpoint, RetrainRequest
    RETRAIN_AVAILABLE = True
    logger.info("‚úÖ M√≥dulo de retreinamento carregado")
except ImportError as e:
    RETRAIN_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è M√≥dulo de retreinamento n√£o dispon√≠vel: {e}")

# Configura√ß√µes do modelo
MODEL_PATH = "best_model.keras"
MODEL_URL = "https://drive.google.com/uc?id=1vSIfD3viT5JSxpG4asA8APCwK0JK9Dvu"
CLASS_NAMES = ['cataract', 'diabetic_retinopathy', 'glaucoma', 'normal']
EXPECTED_MODEL_SIZE = 169_000_000  # 169MB em bytes
MODEL_HASH_FILE = "model_hash.txt"

# Poss√≠veis localiza√ß√µes de modelos locais
POSSIBLE_MODEL_PATHS = [
    "best_model.keras",
    "best_model.h5",
    "model.keras",
    "model.h5",
    "eye_disease_model.keras",
    "eye_disease_model.h5",
    "models/best_model.keras",
    "models/best_model.h5",
    "../models/best_model.keras",
    "../models/best_model.h5"
]

# Vari√°veis globais para o modelo
model = None
model_loading = False
model_load_error = None
model_load_attempts = 0
MAX_LOAD_ATTEMPTS = 3

app = FastAPI(
    title="Eye Disease Classifier API",
    description="API para classifica√ß√£o de doen√ßas oculares usando deep learning",
    version="1.0.0"
)

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def get_memory_usage():
    """Retorna uso atual de mem√≥ria"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return {
        "rss_mb": round(memory_info.rss / 1024 / 1024, 2),
        "vms_mb": round(memory_info.vms / 1024 / 1024, 2),
        "percent": round(process.memory_percent(), 2)
    }

def calculate_file_hash(filepath):
    """Calcula hash MD5 do arquivo para verificar integridade"""
    hash_md5 = hashlib.md5()
    try:
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        logger.error(f"Erro ao calcular hash: {e}")
        return None

def verify_model_integrity():
    """Verifica se o modelo est√° √≠ntegro - flex√≠vel para modelos locais"""
    if not os.path.exists(MODEL_PATH):
        return False, "Arquivo n√£o existe"

    file_size = os.path.getsize(MODEL_PATH)

    # Verifica√ß√£o mais flex√≠vel para modelos locais
    min_size = 50_000_000  # 50MB m√≠nimo (mais flex√≠vel)
    max_size = 300_000_000  # 300MB m√°ximo (para detectar arquivos corrompidos)

    if file_size < min_size:
        return False, f"Arquivo muito pequeno: {file_size / 1_000_000:.1f}MB (m√≠nimo: {min_size / 1_000_000:.1f}MB)"

    if file_size > max_size:
        return False, f"Arquivo muito grande: {file_size / 1_000_000:.1f}MB (m√°ximo: {max_size / 1_000_000:.1f}MB)"

    # Verificar se √© um arquivo Keras v√°lido (extens√£o e estrutura b√°sica)
    if not MODEL_PATH.endswith(('.keras', '.h5')):
        return False, "Extens√£o de arquivo inv√°lida (deve ser .keras ou .h5)"

    # Verificar hash apenas se dispon√≠vel (n√£o obrigat√≥rio para modelos locais)
    if os.path.exists(MODEL_HASH_FILE):
        try:
            with open(MODEL_HASH_FILE, 'r') as f:
                expected_hash = f.read().strip()
            current_hash = calculate_file_hash(MODEL_PATH)
            if current_hash and current_hash != expected_hash:
                logger.warning("Hash do arquivo n√£o confere, mas continuando (modelo local)")
        except Exception as e:
            logger.warning(f"Erro ao verificar hash: {e}")

    return True, f"Arquivo v√°lido: {file_size / 1_000_000:.1f}MB"

def find_local_model():
    """Procura por modelos locais em diferentes localiza√ß√µes"""
    logger.info("üîç Procurando por modelos locais...")

    for model_path in POSSIBLE_MODEL_PATHS:
        if os.path.exists(model_path):
            file_size = os.path.getsize(model_path)
            logger.info(f"üìÅ Modelo encontrado: {model_path} ({file_size / 1_000_000:.1f}MB)")

            # Verificar se parece ser um modelo v√°lido
            if file_size > 50_000_000:  # Pelo menos 50MB
                logger.info(f"‚úÖ Modelo local detectado: {model_path}")
                return model_path
            else:
                logger.warning(f"‚ö†Ô∏è Arquivo muito pequeno, ignorando: {model_path}")

    logger.info("‚ùå Nenhum modelo local encontrado")
    return None

def setup_model_path():
    """Configura o caminho do modelo, priorizando modelos locais"""
    global MODEL_PATH

    # Primeiro, tentar encontrar modelo local
    local_model = find_local_model()
    if local_model:
        MODEL_PATH = local_model
        logger.info(f"üéØ Usando modelo local: {MODEL_PATH}")
        return True

    # Se n√£o encontrou, usar o caminho padr√£o
    logger.info(f"üì¶ Usando caminho padr√£o: {MODEL_PATH}")
    return False

def download_model_with_retry(max_retries=3):
    """Baixa o modelo com retry autom√°tico"""
    for attempt in range(max_retries):
        try:
            logger.info(f"üîΩ Tentativa {attempt + 1}/{max_retries} de download...")

            # Limpar arquivos parciais
            if os.path.exists(MODEL_PATH):
                os.remove(MODEL_PATH)

            # Download com timeout
            gdown.download(
                MODEL_URL,
                MODEL_PATH,
                quiet=False,
                fuzzy=True
            )

            # Verificar integridade
            is_valid, message = verify_model_integrity()
            if is_valid:
                logger.info(f"‚úÖ Download bem-sucedido! {message}")
                # Salvar hash para verifica√ß√µes futuras
                file_hash = calculate_file_hash(MODEL_PATH)
                if file_hash:
                    with open(MODEL_HASH_FILE, 'w') as f:
                        f.write(file_hash)
                return True
            else:
                logger.warning(f"‚ö†Ô∏è Download inv√°lido: {message}")
                if attempt < max_retries - 1:
                    time.sleep(5)  # Aguardar antes da pr√≥xima tentativa

        except Exception as e:
            logger.error(f"‚ùå Erro no download (tentativa {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(5)

    return False

def configure_tensorflow():
    """Configura TensorFlow para uso APENAS de CPU (sem GPU)"""
    try:
        import tensorflow as tf

        # FOR√áAR USO APENAS DE CPU
        tf.config.set_visible_devices([], 'GPU')

        # Configurar threads de forma muito conservadora
        tf.config.threading.set_intra_op_parallelism_threads(1)
        tf.config.threading.set_inter_op_parallelism_threads(1)

        # Configurar uso de mem√≥ria de forma conservadora
        tf.config.experimental.enable_memory_growth = False

        # Verificar se GPU foi realmente desabilitada
        gpus = tf.config.list_physical_devices('GPU')
        visible_gpus = tf.config.get_visible_devices('GPU')

        logger.info(f"üñ•Ô∏è GPUs f√≠sicas detectadas: {len(gpus)}")
        logger.info(f"üñ•Ô∏è GPUs vis√≠veis (deve ser 0): {len(visible_gpus)}")
        logger.info("‚úÖ TensorFlow configurado para CPU APENAS")

        return True

    except Exception as e:
        logger.error(f"‚ùå Erro ao configurar TensorFlow: {e}")
        return False

def load_model_safe():
    """Carrega o modelo com configura√ß√µes de seguran√ßa m√°xima"""
    try:
        logger.info("üîÑ Iniciando carregamento seguro do modelo...")

        # Configurar TensorFlow ANTES de qualquer importa√ß√£o
        configure_tensorflow()

        # Importar TensorFlow apenas quando necess√°rio e ap√≥s configura√ß√£o
        logger.info("üì¶ Importando TensorFlow/Keras...")
        from keras.models import load_model
        from keras.applications.inception_v3 import preprocess_input

        # Verificar mem√≥ria antes do carregamento
        memory_before = get_memory_usage()
        logger.info(f"üíæ Mem√≥ria antes do carregamento: {memory_before['rss_mb']}MB")

        # Carregar modelo com timeout impl√≠cito
        logger.info("üîÑ Carregando modelo (pode demorar alguns minutos)...")
        logger.info("‚ö†Ô∏è Usando APENAS CPU para m√°xima estabilidade")

        try:
            model = load_model(MODEL_PATH, compile=False)  # N√£o compilar para economizar mem√≥ria
            logger.info("‚úÖ Modelo carregado sem compila√ß√£o")

            # Compilar manualmente de forma mais segura
            logger.info("üîß Compilando modelo...")
            model.compile(
                optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
            logger.info("‚úÖ Modelo compilado com sucesso")

        except Exception as load_error:
            logger.error(f"‚ùå Erro no carregamento: {load_error}")
            # Tentar carregamento alternativo
            logger.info("üîÑ Tentando carregamento alternativo...")
            model = load_model(MODEL_PATH)

        # Verificar mem√≥ria ap√≥s carregamento
        memory_after = get_memory_usage()
        logger.info(f"üíæ Mem√≥ria ap√≥s carregamento: {memory_after['rss_mb']}MB")
        logger.info(f"üìà Incremento de mem√≥ria: {memory_after['rss_mb'] - memory_before['rss_mb']:.1f}MB")

        # Fazer uma predi√ß√£o de teste simples
        logger.info("üß™ Testando modelo com predi√ß√£o simples...")
        test_input = np.random.random((1, 256, 256, 3)).astype(np.float32)
        test_prediction = model.predict(test_input, verbose=0)

        logger.info("‚úÖ Modelo carregado e testado com sucesso!")
        logger.info(f"üìä Modelo pronto para classificar {len(CLASS_NAMES)} classes")

        return model

    except Exception as e:
        logger.error(f"‚ùå Erro ao carregar modelo: {e}")
        logger.error(f"üîç Tipo do erro: {type(e).__name__}")

        # For√ßar limpeza de mem√≥ria
        logger.info("üßπ Limpando mem√≥ria...")
        gc.collect()

        # Log de mem√≥ria ap√≥s erro
        memory_after_error = get_memory_usage()
        logger.info(f"üíæ Mem√≥ria ap√≥s erro: {memory_after_error['rss_mb']}MB")

        raise e

def download_and_load_model():
    """Carrega modelo local ou baixa se necess√°rio - com fallback robusto"""
    global model, model_loading, model_load_error, model_load_attempts

    if model_loading:
        logger.info("‚è≥ Carregamento j√° em andamento...")
        return

    model_loading = True
    model_load_attempts += 1

    try:
        logger.info(f"üîÑ Iniciando carregamento do modelo (tentativa {model_load_attempts}/{MAX_LOAD_ATTEMPTS})...")

        # PRIORIDADE 0: Detectar modelos locais automaticamente
        setup_model_path()

        # PRIORIDADE 1: Verificar se existe modelo local v√°lido
        if os.path.exists(MODEL_PATH):
            logger.info("üìÅ Modelo local encontrado, verificando integridade...")
            is_valid, message = verify_model_integrity()

            if is_valid:
                logger.info(f"‚úÖ Modelo local v√°lido: {message}")
                # Tentar carregar o modelo local
                try:
                    model = load_model_safe()
                    model_load_error = None
                    logger.info("üéâ Modelo local carregado com sucesso!")
                    return
                except Exception as load_error:
                    logger.warning(f"‚ö†Ô∏è Erro ao carregar modelo local: {load_error}")
                    logger.info("üîÑ Tentando redownload...")
                    # Continuar para download se carregamento falhar
            else:
                logger.warning(f"‚ö†Ô∏è Modelo local inv√°lido: {message}")
                logger.info("üóëÔ∏è Removendo modelo corrompido...")
                try:
                    os.remove(MODEL_PATH)
                except:
                    pass

        # PRIORIDADE 2: Tentar baixar modelo se n√£o existe ou √© inv√°lido
        if not os.path.exists(MODEL_PATH):
            logger.info("üîΩ Modelo local n√£o encontrado, tentando download...")
            logger.info("üì¶ Tamanho esperado: ~169MB")
            logger.info("‚è≥ Isso pode levar alguns minutos...")

            try:
                if download_model_with_retry():
                    logger.info("‚úÖ Download conclu√≠do, carregando modelo...")
                    model = load_model_safe()
                    model_load_error = None
                    logger.info("üéâ Modelo baixado e carregado com sucesso!")
                    return
                else:
                    raise Exception("Falha no download ap√≥s m√∫ltiplas tentativas")
            except Exception as download_error:
                logger.error(f"‚ùå Erro no download: {download_error}")
                # Continuar para fallback

        # PRIORIDADE 3: Tentar carregar modelo mesmo com problemas de integridade
        if os.path.exists(MODEL_PATH):
            logger.info("üîÑ Tentando carregar modelo mesmo com poss√≠veis problemas...")
            try:
                model = load_model_safe()
                model_load_error = None
                logger.info("üéâ Modelo carregado com sucesso (ignorando verifica√ß√£o de integridade)!")
                return
            except Exception as final_load_error:
                logger.error(f"‚ùå Falha final no carregamento: {final_load_error}")

        # Se chegou aqui, todas as tentativas falharam
        raise Exception("Todas as tentativas de carregamento falharam")

    except Exception as e:
        error_msg = f"Erro no carregamento do modelo: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        model_load_error = error_msg

        # FALLBACK: Modo demo
        if model_load_attempts < MAX_LOAD_ATTEMPTS:
            logger.info(f"üîÑ Tentativa {model_load_attempts + 1}/{MAX_LOAD_ATTEMPTS} ser√° feita automaticamente")
            model = "demo_mode"
        else:
            logger.error("‚ùå M√°ximo de tentativas atingido. Continuando em modo demo.")
            logger.info("üé≠ Modo demo permite testar a API sem modelo real")
            model = "demo_mode"

    finally:
        model_loading = False

@app.on_event("startup")
async def startup_event():
    """Evento executado na inicializa√ß√£o da API"""
    logger.info("üöÄ Iniciando API do Eye Disease Classifier...")
    logger.info("üìç API estar√° dispon√≠vel para healthcheck imediatamente")
    logger.info("üîÑ Modelo ser√° carregado em background...")

    # Mostrar informa√ß√µes do sistema
    memory_info = get_memory_usage()
    logger.info(f"üíæ Mem√≥ria inicial: {memory_info['rss_mb']}MB ({memory_info['percent']}%)")

    # Carregar modelo em thread separada para n√£o bloquear a API
    def load_model_background():
        try:
            download_and_load_model()
        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico no carregamento do modelo: {e}")
            # Garantir que a API continue funcionando em modo demo
            global model
            model = "demo_mode"

    thread = threading.Thread(target=load_model_background, daemon=True)
    thread.start()

    logger.info("‚úÖ API iniciada com sucesso!")

@app.get("/")
async def root():
    """Endpoint raiz com informa√ß√µes da API"""
    return {
        "message": "Eye Disease Classifier API",
        "version": "1.0.0",
        "status": "running",
        "model_loaded": model is not None,
        "classes": CLASS_NAMES
    }

@app.get("/health")
async def health_check():
    """Endpoint para verificar sa√∫de da API"""
    global model, model_loading, model_load_error, model_load_attempts

    memory_info = get_memory_usage()

    base_response = {
        "memory_usage": memory_info,
        "load_attempts": model_load_attempts,
        "max_attempts": MAX_LOAD_ATTEMPTS
    }

    if model is None:
        if model_loading:
            return {
                **base_response,
                "status": "loading",
                "message": "Modelo sendo carregado...",
                "model_loaded": False,
                "loading": True
            }
        else:
            return {
                **base_response,
                "status": "starting",
                "message": "API iniciando, modelo ser√° carregado...",
                "model_loaded": False,
                "error": model_load_error
            }
    elif model == "demo_mode":
        return {
            **base_response,
            "status": "healthy",
            "message": "API funcionando em modo demo",
            "model_loaded": False,
            "demo_mode": True,
            "error": model_load_error
        }
    else:
        return {
            **base_response,
            "status": "healthy",
            "message": "API funcionando com modelo carregado",
            "model_loaded": True
        }

def preprocess_image(image: Image.Image) -> np.ndarray:
    """Preprocessa a imagem para o modelo"""
    try:
        # Converter para RGB se necess√°rio
        if image.mode != 'RGB':
            image = image.convert('RGB')

        # Redimensionar para 256x256
        image = image.resize((256, 256))

        # Converter para array numpy
        img_array = np.array(image)

        # Preprocessar usando inception_v3
        from keras.applications.inception_v3 import preprocess_input
        img_array = preprocess_input(img_array)

        # Adicionar dimens√£o do batch
        img_array = np.expand_dims(img_array, axis=0)

        return img_array
    except Exception as e:
        logger.error(f"Erro no preprocessamento da imagem: {e}")
        raise e

@app.post("/predict")
async def predict_disease(file: UploadFile = File(...)) -> Dict:
    """
    Endpoint para predi√ß√£o de doen√ßas oculares

    Args:
        file: Arquivo de imagem (JPG, JPEG, PNG)

    Returns:
        Dict com predi√ß√£o e confian√ßa
    """
    if model is None:
        raise HTTPException(status_code=500, detail="Modelo n√£o carregado")

    # Verificar tipo de arquivo
    if file.content_type not in ["image/jpeg", "image/jpg", "image/png"]:
        raise HTTPException(
            status_code=400,
            detail="Tipo de arquivo n√£o suportado. Use JPG, JPEG ou PNG."
        )

    try:
        # Ler e processar imagem
        image_data = await file.read()
        image = Image.open(io.BytesIO(image_data))

        # Verificar se est√° em modo demo
        if model == "demo_mode":
            logger.info("üé≠ Executando predi√ß√£o em modo demo")

            # Simular predi√ß√£o com valores aleat√≥rios mas realistas
            import random
            random.seed(hash(str(image_data)) % 1000)  # Seed baseado na imagem para consist√™ncia

            # Gerar probabilidades simuladas
            probs = [random.uniform(0.1, 0.9) for _ in CLASS_NAMES]
            total = sum(probs)
            probs = [p/total for p in probs]  # Normalizar para somar 1

            predicted_class_idx = probs.index(max(probs))
            predicted_class = CLASS_NAMES[predicted_class_idx]
            confidence = max(probs)

            all_predictions = {
                class_name: float(prob)
                for class_name, prob in zip(CLASS_NAMES, probs)
            }

            return {
                "predicted_class": predicted_class,
                "confidence": confidence,
                "all_predictions": all_predictions,
                "status": "demo_mode",
                "message": "Esta √© uma predi√ß√£o simulada. O modelo real n√£o est√° carregado."
            }

        # Preprocessar imagem para o modelo real
        processed_image = preprocess_image(image)

        # Fazer predi√ß√£o com o modelo real
        prediction = model.predict(processed_image)[0]

        # Obter classe predita e confian√ßa
        predicted_class_idx = np.argmax(prediction)
        predicted_class = CLASS_NAMES[predicted_class_idx]
        confidence = float(np.max(prediction))

        # Criar resposta com todas as probabilidades
        all_predictions = {
            class_name: float(prob)
            for class_name, prob in zip(CLASS_NAMES, prediction)
        }

        return {
            "predicted_class": predicted_class,
            "confidence": confidence,
            "all_predictions": all_predictions,
            "status": "success"
        }

    except Exception as e:
        logger.error(f"Erro na predi√ß√£o: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro na predi√ß√£o: {str(e)}")

@app.get("/classes")
async def get_classes() -> Dict[str, List[str]]:
    """Retorna as classes dispon√≠veis"""
    return {"classes": CLASS_NAMES}

@app.get("/status")
async def get_status():
    """Retorna status detalhado da API e modelo"""
    global model, model_loading, model_load_error, model_load_attempts

    # Verificar se arquivo do modelo existe
    model_exists = os.path.exists(MODEL_PATH)
    model_size = 0
    model_hash = None

    if model_exists:
        model_size = os.path.getsize(MODEL_PATH)
        model_hash = calculate_file_hash(MODEL_PATH)

    # Verificar integridade
    is_valid, integrity_message = verify_model_integrity() if model_exists else (False, "Arquivo n√£o existe")

    memory_info = get_memory_usage()

    return {
        "api_status": "running",
        "model_status": {
            "loaded": model is not None and model != "demo_mode",
            "demo_mode": model == "demo_mode",
            "loading": model_loading,
            "file_exists": model_exists,
            "file_size_mb": round(model_size / 1_000_000, 2) if model_size > 0 else 0,
            "expected_size_mb": round(EXPECTED_MODEL_SIZE / 1_000_000, 2),
            "integrity_valid": is_valid,
            "integrity_message": integrity_message,
            "file_hash": model_hash[:16] + "..." if model_hash else None,
            "load_attempts": model_load_attempts,
            "max_attempts": MAX_LOAD_ATTEMPTS,
            "last_error": model_load_error
        },
        "system": {
            "memory_usage": memory_info,
            "tensorflow_configured": True
        },
        "endpoints": {
            "health": "/health",
            "predict": "/predict",
            "docs": "/docs",
            "classes": "/classes",
            "reload_model": "/reload-model",
            "memory": "/memory"
        }
    }

@app.post("/reload-model")
async def reload_model():
    """For√ßa o recarregamento do modelo"""
    global model, model_loading, model_load_error, model_load_attempts

    if model_loading:
        return {
            "status": "error",
            "message": "Carregamento j√° em andamento"
        }

    # Reset das vari√°veis
    model = None
    model_load_error = None
    model_load_attempts = 0

    # Iniciar carregamento em background
    def reload_background():
        download_and_load_model()

    thread = threading.Thread(target=reload_background, daemon=True)
    thread.start()

    return {
        "status": "success",
        "message": "Recarregamento do modelo iniciado"
    }

@app.get("/memory")
async def get_memory_info():
    """Retorna informa√ß√µes detalhadas de mem√≥ria"""
    memory_info = get_memory_usage()

    # Informa√ß√µes do sistema
    system_memory = psutil.virtual_memory()

    return {
        "process": memory_info,
        "system": {
            "total_mb": round(system_memory.total / 1024 / 1024, 2),
            "available_mb": round(system_memory.available / 1024 / 1024, 2),
            "used_mb": round(system_memory.used / 1024 / 1024, 2),
            "percent": round(system_memory.percent, 2)
        },
        "model_status": {
            "loaded": model is not None and model != "demo_mode",
            "demo_mode": model == "demo_mode"
        }
    }

# === ENDPOINTS DE RETREINAMENTO ===

if RETRAIN_AVAILABLE:
    try:
        add_retrain_endpoint(app, model)
        logger.info("‚úÖ Endpoints de retreinamento adicionados")
    except Exception as e:
        logger.error(f"‚ùå Erro ao adicionar endpoints de retreinamento: {e}")
else:
    @app.post("/retrain")
    async def retrain_unavailable():
        """Endpoint de retreinamento n√£o dispon√≠vel"""
        raise HTTPException(
            status_code=503,
            detail="Retreinamento n√£o dispon√≠vel. M√≥dulo n√£o carregado."
        )

    @app.get("/retrain/status")
    async def retrain_status_unavailable():
        """Status do retreinamento quando n√£o dispon√≠vel"""
        return {
            "retrain_available": False,
            "error": "M√≥dulo de retreinamento n√£o carregado",
            "model_loaded": model is not None and model != "demo_mode"
        }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
