from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import numpy as np
import gdown
import os
import io
import uvicorn
from typing import Dict, List
import logging
import threading
import time
import hashlib
import gc
import psutil

# Configurar TensorFlow antes de importar - FOR√áAR CPU APENAS
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suprimir todos os logs do TensorFlow
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Desabilitar completamente CUDA/GPU
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false'  # Desabilitar GPU
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Desabilitar otimiza√ß√µes OneDNN
os.environ['TF_DISABLE_MKL'] = '1'  # Desabilitar MKL que pode causar problemas
os.environ['TF_NUM_INTEROP_THREADS'] = '1'  # Limitar threads
os.environ['TF_NUM_INTRAOP_THREADS'] = '1'  # Limitar threads
os.environ['OMP_NUM_THREADS'] = '1'  # Limitar OpenMP threads

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configura√ß√µes do modelo
MODEL_PATH = "best_model.keras"
MODEL_URL = "https://drive.google.com/uc?id=1vSIfD3viT5JSxpG4asA8APCwK0JK9Dvu"
CLASS_NAMES = ['cataract', 'diabetic_retinopathy', 'glaucoma', 'normal']
EXPECTED_MODEL_SIZE = 169_000_000  # 169MB em bytes
MODEL_HASH_FILE = "model_hash.txt"

# Vari√°veis globais para o modelo
model = None
model_loading = False
model_load_error = None
model_load_attempts = 0
MAX_LOAD_ATTEMPTS = 3

app = FastAPI(
    title="Eye Disease Classifier API",
    description="API para classifica√ß√£o de doen√ßas oculares usando deep learning",
    version="1.0.0"
)

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def get_memory_usage():
    """Retorna uso atual de mem√≥ria"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return {
        "rss_mb": round(memory_info.rss / 1024 / 1024, 2),
        "vms_mb": round(memory_info.vms / 1024 / 1024, 2),
        "percent": round(process.memory_percent(), 2)
    }

def calculate_file_hash(filepath):
    """Calcula hash MD5 do arquivo para verificar integridade"""
    hash_md5 = hashlib.md5()
    try:
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        logger.error(f"Erro ao calcular hash: {e}")
        return None

def verify_model_integrity():
    """Verifica se o modelo baixado est√° √≠ntegro"""
    if not os.path.exists(MODEL_PATH):
        return False, "Arquivo n√£o existe"

    file_size = os.path.getsize(MODEL_PATH)
    if file_size < EXPECTED_MODEL_SIZE * 0.9:  # Pelo menos 90% do tamanho esperado
        return False, f"Arquivo muito pequeno: {file_size / 1_000_000:.1f}MB (esperado: {EXPECTED_MODEL_SIZE / 1_000_000:.1f}MB)"

    # Verificar hash se dispon√≠vel
    if os.path.exists(MODEL_HASH_FILE):
        try:
            with open(MODEL_HASH_FILE, 'r') as f:
                expected_hash = f.read().strip()
            current_hash = calculate_file_hash(MODEL_PATH)
            if current_hash and current_hash != expected_hash:
                return False, "Hash do arquivo n√£o confere"
        except Exception as e:
            logger.warning(f"Erro ao verificar hash: {e}")

    return True, "Arquivo √≠ntegro"

def download_model_with_retry(max_retries=3):
    """Baixa o modelo com retry autom√°tico"""
    for attempt in range(max_retries):
        try:
            logger.info(f"üîΩ Tentativa {attempt + 1}/{max_retries} de download...")

            # Limpar arquivos parciais
            if os.path.exists(MODEL_PATH):
                os.remove(MODEL_PATH)

            # Download com timeout
            gdown.download(
                MODEL_URL,
                MODEL_PATH,
                quiet=False,
                fuzzy=True
            )

            # Verificar integridade
            is_valid, message = verify_model_integrity()
            if is_valid:
                logger.info(f"‚úÖ Download bem-sucedido! {message}")
                # Salvar hash para verifica√ß√µes futuras
                file_hash = calculate_file_hash(MODEL_PATH)
                if file_hash:
                    with open(MODEL_HASH_FILE, 'w') as f:
                        f.write(file_hash)
                return True
            else:
                logger.warning(f"‚ö†Ô∏è Download inv√°lido: {message}")
                if attempt < max_retries - 1:
                    time.sleep(5)  # Aguardar antes da pr√≥xima tentativa

        except Exception as e:
            logger.error(f"‚ùå Erro no download (tentativa {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(5)

    return False

def configure_tensorflow():
    """Configura TensorFlow para uso APENAS de CPU (sem GPU)"""
    try:
        import tensorflow as tf

        # FOR√áAR USO APENAS DE CPU
        tf.config.set_visible_devices([], 'GPU')

        # Configurar threads de forma muito conservadora
        tf.config.threading.set_intra_op_parallelism_threads(1)
        tf.config.threading.set_inter_op_parallelism_threads(1)

        # Configurar uso de mem√≥ria de forma conservadora
        tf.config.experimental.enable_memory_growth = False

        # Verificar se GPU foi realmente desabilitada
        gpus = tf.config.list_physical_devices('GPU')
        visible_gpus = tf.config.get_visible_devices('GPU')

        logger.info(f"üñ•Ô∏è GPUs f√≠sicas detectadas: {len(gpus)}")
        logger.info(f"üñ•Ô∏è GPUs vis√≠veis (deve ser 0): {len(visible_gpus)}")
        logger.info("‚úÖ TensorFlow configurado para CPU APENAS")

        return True

    except Exception as e:
        logger.error(f"‚ùå Erro ao configurar TensorFlow: {e}")
        return False

def load_model_safe():
    """Carrega o modelo com configura√ß√µes de seguran√ßa m√°xima"""
    try:
        logger.info("üîÑ Iniciando carregamento seguro do modelo...")

        # Configurar TensorFlow ANTES de qualquer importa√ß√£o
        configure_tensorflow()

        # Importar TensorFlow apenas quando necess√°rio e ap√≥s configura√ß√£o
        logger.info("üì¶ Importando TensorFlow/Keras...")
        from keras.models import load_model
        from keras.applications.inception_v3 import preprocess_input

        # Verificar mem√≥ria antes do carregamento
        memory_before = get_memory_usage()
        logger.info(f"üíæ Mem√≥ria antes do carregamento: {memory_before['rss_mb']}MB")

        # Carregar modelo com timeout impl√≠cito
        logger.info("üîÑ Carregando modelo (pode demorar alguns minutos)...")
        logger.info("‚ö†Ô∏è Usando APENAS CPU para m√°xima estabilidade")

        try:
            model = load_model(MODEL_PATH, compile=False)  # N√£o compilar para economizar mem√≥ria
            logger.info("‚úÖ Modelo carregado sem compila√ß√£o")

            # Compilar manualmente de forma mais segura
            logger.info("üîß Compilando modelo...")
            model.compile(
                optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
            logger.info("‚úÖ Modelo compilado com sucesso")

        except Exception as load_error:
            logger.error(f"‚ùå Erro no carregamento: {load_error}")
            # Tentar carregamento alternativo
            logger.info("üîÑ Tentando carregamento alternativo...")
            model = load_model(MODEL_PATH)

        # Verificar mem√≥ria ap√≥s carregamento
        memory_after = get_memory_usage()
        logger.info(f"üíæ Mem√≥ria ap√≥s carregamento: {memory_after['rss_mb']}MB")
        logger.info(f"üìà Incremento de mem√≥ria: {memory_after['rss_mb'] - memory_before['rss_mb']:.1f}MB")

        # Fazer uma predi√ß√£o de teste simples
        logger.info("üß™ Testando modelo com predi√ß√£o simples...")
        test_input = np.random.random((1, 256, 256, 3)).astype(np.float32)
        test_prediction = model.predict(test_input, verbose=0)

        logger.info("‚úÖ Modelo carregado e testado com sucesso!")
        logger.info(f"üìä Modelo pronto para classificar {len(CLASS_NAMES)} classes")

        return model

    except Exception as e:
        logger.error(f"‚ùå Erro ao carregar modelo: {e}")
        logger.error(f"üîç Tipo do erro: {type(e).__name__}")

        # For√ßar limpeza de mem√≥ria
        logger.info("üßπ Limpando mem√≥ria...")
        gc.collect()

        # Log de mem√≥ria ap√≥s erro
        memory_after_error = get_memory_usage()
        logger.info(f"üíæ Mem√≥ria ap√≥s erro: {memory_after_error['rss_mb']}MB")

        raise e

def download_and_load_model():
    """Baixa e carrega o modelo com mecanismo robusto"""
    global model, model_loading, model_load_error, model_load_attempts

    if model_loading:
        logger.info("‚è≥ Carregamento j√° em andamento...")
        return

    model_loading = True
    model_load_attempts += 1

    try:
        logger.info(f"üîÑ Iniciando carregamento do modelo (tentativa {model_load_attempts}/{MAX_LOAD_ATTEMPTS})...")

        # Verificar se o modelo j√° existe e est√° √≠ntegro
        if os.path.exists(MODEL_PATH):
            is_valid, message = verify_model_integrity()
            if is_valid:
                logger.info(f"‚úÖ Modelo existente verificado: {message}")
            else:
                logger.warning(f"‚ö†Ô∏è Modelo existente inv√°lido: {message}")
                os.remove(MODEL_PATH)

        # Baixar modelo se necess√°rio
        if not os.path.exists(MODEL_PATH):
            logger.info("üîΩ Baixando modelo do Google Drive...")
            logger.info("üì¶ Tamanho esperado: ~169MB")
            logger.info("‚è≥ Isso pode levar alguns minutos...")

            if not download_model_with_retry():
                raise Exception("Falha no download ap√≥s m√∫ltiplas tentativas")

        # Carregar modelo
        model = load_model_safe()
        model_load_error = None

        logger.info("üéâ Modelo carregado com sucesso!")

    except Exception as e:
        error_msg = f"Erro no carregamento do modelo: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        model_load_error = error_msg

        if model_load_attempts < MAX_LOAD_ATTEMPTS:
            logger.info(f"üîÑ Tentativa {model_load_attempts + 1}/{MAX_LOAD_ATTEMPTS} ser√° feita automaticamente")
            model = "demo_mode"
        else:
            logger.error("‚ùå M√°ximo de tentativas atingido. Continuando em modo demo.")
            model = "demo_mode"

    finally:
        model_loading = False

@app.on_event("startup")
async def startup_event():
    """Evento executado na inicializa√ß√£o da API"""
    logger.info("üöÄ Iniciando API do Eye Disease Classifier...")
    logger.info("üìç API estar√° dispon√≠vel para healthcheck imediatamente")
    logger.info("üîÑ Modelo ser√° carregado em background...")

    # Mostrar informa√ß√µes do sistema
    memory_info = get_memory_usage()
    logger.info(f"üíæ Mem√≥ria inicial: {memory_info['rss_mb']}MB ({memory_info['percent']}%)")

    # Carregar modelo em thread separada para n√£o bloquear a API
    def load_model_background():
        try:
            download_and_load_model()
        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico no carregamento do modelo: {e}")
            # Garantir que a API continue funcionando em modo demo
            global model
            model = "demo_mode"

    thread = threading.Thread(target=load_model_background, daemon=True)
    thread.start()

    logger.info("‚úÖ API iniciada com sucesso!")

@app.get("/")
async def root():
    """Endpoint raiz com informa√ß√µes da API"""
    return {
        "message": "Eye Disease Classifier API",
        "version": "1.0.0",
        "status": "running",
        "model_loaded": model is not None,
        "classes": CLASS_NAMES
    }

@app.get("/health")
async def health_check():
    """Endpoint para verificar sa√∫de da API"""
    global model, model_loading, model_load_error, model_load_attempts

    memory_info = get_memory_usage()

    base_response = {
        "memory_usage": memory_info,
        "load_attempts": model_load_attempts,
        "max_attempts": MAX_LOAD_ATTEMPTS
    }

    if model is None:
        if model_loading:
            return {
                **base_response,
                "status": "loading",
                "message": "Modelo sendo carregado...",
                "model_loaded": False,
                "loading": True
            }
        else:
            return {
                **base_response,
                "status": "starting",
                "message": "API iniciando, modelo ser√° carregado...",
                "model_loaded": False,
                "error": model_load_error
            }
    elif model == "demo_mode":
        return {
            **base_response,
            "status": "healthy",
            "message": "API funcionando em modo demo",
            "model_loaded": False,
            "demo_mode": True,
            "error": model_load_error
        }
    else:
        return {
            **base_response,
            "status": "healthy",
            "message": "API funcionando com modelo carregado",
            "model_loaded": True
        }

def preprocess_image(image: Image.Image) -> np.ndarray:
    """Preprocessa a imagem para o modelo"""
    try:
        # Converter para RGB se necess√°rio
        if image.mode != 'RGB':
            image = image.convert('RGB')

        # Redimensionar para 256x256
        image = image.resize((256, 256))

        # Converter para array numpy
        img_array = np.array(image)

        # Preprocessar usando inception_v3
        from keras.applications.inception_v3 import preprocess_input
        img_array = preprocess_input(img_array)

        # Adicionar dimens√£o do batch
        img_array = np.expand_dims(img_array, axis=0)

        return img_array
    except Exception as e:
        logger.error(f"Erro no preprocessamento da imagem: {e}")
        raise e

@app.post("/predict")
async def predict_disease(file: UploadFile = File(...)) -> Dict:
    """
    Endpoint para predi√ß√£o de doen√ßas oculares

    Args:
        file: Arquivo de imagem (JPG, JPEG, PNG)

    Returns:
        Dict com predi√ß√£o e confian√ßa
    """
    if model is None:
        raise HTTPException(status_code=500, detail="Modelo n√£o carregado")

    # Verificar tipo de arquivo
    if file.content_type not in ["image/jpeg", "image/jpg", "image/png"]:
        raise HTTPException(
            status_code=400,
            detail="Tipo de arquivo n√£o suportado. Use JPG, JPEG ou PNG."
        )

    try:
        # Ler e processar imagem
        image_data = await file.read()
        image = Image.open(io.BytesIO(image_data))

        # Verificar se est√° em modo demo
        if model == "demo_mode":
            logger.info("üé≠ Executando predi√ß√£o em modo demo")

            # Simular predi√ß√£o com valores aleat√≥rios mas realistas
            import random
            random.seed(hash(str(image_data)) % 1000)  # Seed baseado na imagem para consist√™ncia

            # Gerar probabilidades simuladas
            probs = [random.uniform(0.1, 0.9) for _ in CLASS_NAMES]
            total = sum(probs)
            probs = [p/total for p in probs]  # Normalizar para somar 1

            predicted_class_idx = probs.index(max(probs))
            predicted_class = CLASS_NAMES[predicted_class_idx]
            confidence = max(probs)

            all_predictions = {
                class_name: float(prob)
                for class_name, prob in zip(CLASS_NAMES, probs)
            }

            return {
                "predicted_class": predicted_class,
                "confidence": confidence,
                "all_predictions": all_predictions,
                "status": "demo_mode",
                "message": "Esta √© uma predi√ß√£o simulada. O modelo real n√£o est√° carregado."
            }

        # Preprocessar imagem para o modelo real
        processed_image = preprocess_image(image)

        # Fazer predi√ß√£o com o modelo real
        prediction = model.predict(processed_image)[0]

        # Obter classe predita e confian√ßa
        predicted_class_idx = np.argmax(prediction)
        predicted_class = CLASS_NAMES[predicted_class_idx]
        confidence = float(np.max(prediction))

        # Criar resposta com todas as probabilidades
        all_predictions = {
            class_name: float(prob)
            for class_name, prob in zip(CLASS_NAMES, prediction)
        }

        return {
            "predicted_class": predicted_class,
            "confidence": confidence,
            "all_predictions": all_predictions,
            "status": "success"
        }

    except Exception as e:
        logger.error(f"Erro na predi√ß√£o: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro na predi√ß√£o: {str(e)}")

@app.get("/classes")
async def get_classes() -> Dict[str, List[str]]:
    """Retorna as classes dispon√≠veis"""
    return {"classes": CLASS_NAMES}

@app.get("/status")
async def get_status():
    """Retorna status detalhado da API e modelo"""
    global model, model_loading, model_load_error, model_load_attempts

    # Verificar se arquivo do modelo existe
    model_exists = os.path.exists(MODEL_PATH)
    model_size = 0
    model_hash = None

    if model_exists:
        model_size = os.path.getsize(MODEL_PATH)
        model_hash = calculate_file_hash(MODEL_PATH)

    # Verificar integridade
    is_valid, integrity_message = verify_model_integrity() if model_exists else (False, "Arquivo n√£o existe")

    memory_info = get_memory_usage()

    return {
        "api_status": "running",
        "model_status": {
            "loaded": model is not None and model != "demo_mode",
            "demo_mode": model == "demo_mode",
            "loading": model_loading,
            "file_exists": model_exists,
            "file_size_mb": round(model_size / 1_000_000, 2) if model_size > 0 else 0,
            "expected_size_mb": round(EXPECTED_MODEL_SIZE / 1_000_000, 2),
            "integrity_valid": is_valid,
            "integrity_message": integrity_message,
            "file_hash": model_hash[:16] + "..." if model_hash else None,
            "load_attempts": model_load_attempts,
            "max_attempts": MAX_LOAD_ATTEMPTS,
            "last_error": model_load_error
        },
        "system": {
            "memory_usage": memory_info,
            "tensorflow_configured": True
        },
        "endpoints": {
            "health": "/health",
            "predict": "/predict",
            "docs": "/docs",
            "classes": "/classes",
            "reload_model": "/reload-model",
            "memory": "/memory"
        }
    }

@app.post("/reload-model")
async def reload_model():
    """For√ßa o recarregamento do modelo"""
    global model, model_loading, model_load_error, model_load_attempts

    if model_loading:
        return {
            "status": "error",
            "message": "Carregamento j√° em andamento"
        }

    # Reset das vari√°veis
    model = None
    model_load_error = None
    model_load_attempts = 0

    # Iniciar carregamento em background
    def reload_background():
        download_and_load_model()

    thread = threading.Thread(target=reload_background, daemon=True)
    thread.start()

    return {
        "status": "success",
        "message": "Recarregamento do modelo iniciado"
    }

@app.get("/memory")
async def get_memory_info():
    """Retorna informa√ß√µes detalhadas de mem√≥ria"""
    memory_info = get_memory_usage()

    # Informa√ß√µes do sistema
    system_memory = psutil.virtual_memory()

    return {
        "process": memory_info,
        "system": {
            "total_mb": round(system_memory.total / 1024 / 1024, 2),
            "available_mb": round(system_memory.available / 1024 / 1024, 2),
            "used_mb": round(system_memory.used / 1024 / 1024, 2),
            "percent": round(system_memory.percent, 2)
        },
        "model_status": {
            "loaded": model is not None and model != "demo_mode",
            "demo_mode": model == "demo_mode"
        }
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
